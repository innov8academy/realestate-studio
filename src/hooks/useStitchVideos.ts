'use client';

import { useState, useCallback } from 'react';
import {
  Input,
  Output,
  VideoSampleSink,
  VideoSampleSource,
  AudioBufferSource,
  BlobSource,
  ALL_FORMATS,
  BufferTarget,
  Mp4OutputFormat,
  getFirstEncodableAudioCodec,
  canEncodeVideo,
} from 'mediabunny';
import type { Rotation } from 'mediabunny';
import { createAvcEncodingConfig, AVC_LEVEL_4_0, AVC_LEVEL_5_1 } from '@/lib/video-encoding';

// Inlined constants from easy-peasy-ease
const DEFAULT_BITRATE = 8_000_000; // 8 Mbps
const MAX_OUTPUT_FPS = 60;

const FALLBACK_WIDTH = 1920;
const FALLBACK_HEIGHT = 1080;
const BASELINE_PIXEL_LIMIT = 1920 * 1080;

const ensureEvenDimension = (value: number): number => {
  if (!Number.isFinite(value) || value <= 0) {
    return 0;
  }
  const even = value % 2 === 0 ? value : value - 1;
  return even > 0 ? even : 2;
};

const normalizeRotation = (value: unknown): Rotation => {
  return value === 0 || value === 90 || value === 180 || value === 270 ? value : 0;
};

const probeVideoMetadata = async (
  blob: Blob
): Promise<{ width: number; height: number; rotation: Rotation; bitrate: number }> => {
  const source = new BlobSource(blob);
  const input = new Input({
    source,
    formats: ALL_FORMATS,
  });
  try {
    const videoTracks = await input.getVideoTracks();
    if (videoTracks.length === 0) {
      throw new Error('No video tracks found while probing dimensions.');
    }
    const track = videoTracks[0];
    const widthCandidate =
      (typeof track.displayWidth === 'number' && track.displayWidth > 0
        ? track.displayWidth
        : track.codedWidth) ?? FALLBACK_WIDTH;
    const heightCandidate =
      (typeof track.displayHeight === 'number' && track.displayHeight > 0
        ? track.displayHeight
        : track.codedHeight) ?? FALLBACK_HEIGHT;

    // Probe bitrate from packet stats
    let bitrate = 0;
    try {
      const packetStats = await track.computePacketStats();
      if (packetStats?.averageBitrate && Number.isFinite(packetStats.averageBitrate)) {
        bitrate = packetStats.averageBitrate;
      }
    } catch (e) {
      console.warn('Failed to compute packet stats for bitrate', e);
    }

    return {
      width: ensureEvenDimension(widthCandidate),
      height: ensureEvenDimension(heightCandidate),
      rotation: normalizeRotation(track.rotation),
      bitrate,
    };
  } finally {
    input.dispose();
  }
};

const determineEncodeParameters = async (
  blobs: Blob[]
): Promise<{ width: number; height: number; rotation: Rotation; maxSourceBitrate: number }> => {
  let maxWidth = 0;
  let maxHeight = 0;
  let maxSourceBitrate = 0;
  let rotation: Rotation | null = null;

  for (let i = 0; i < blobs.length; i++) {
    try {
      const { width, height, rotation: trackRotation, bitrate } = await probeVideoMetadata(blobs[i]);
      maxWidth = Math.max(maxWidth, width);
      maxHeight = Math.max(maxHeight, height);
      maxSourceBitrate = Math.max(maxSourceBitrate, bitrate);
      if (rotation === null) {
        rotation = trackRotation;
      } else if (trackRotation !== rotation) {
        console.warn(
          `Rotation mismatch detected for video ${i + 1} (got ${trackRotation}, expected ${rotation}). Using the first rotation value.`
        );
      }
    } catch (error) {
      console.warn(`Failed to probe metadata for video ${i + 1}`, error);
    }
  }

  if (maxWidth <= 0 || maxHeight <= 0) {
    return {
      width: FALLBACK_WIDTH,
      height: FALLBACK_HEIGHT,
      rotation: rotation ?? (0 as Rotation),
      maxSourceBitrate,
    };
  }

  return {
    width: ensureEvenDimension(maxWidth),
    height: ensureEvenDimension(maxHeight),
    rotation: rotation ?? (0 as Rotation),
    maxSourceBitrate,
  };
};

export interface StitchProgress {
  status: 'idle' | 'processing' | 'complete' | 'error';
  message: string;
  progress: number; // 0-100
  currentVideo?: number; // Which video is being processed (1-indexed)
  totalVideos?: number;
  error?: string;
}

export interface AudioData {
  buffer: AudioBuffer;
  duration: number;
}

/**
 * Check if the device encoder supports AVC encoding
 */
export async function checkEncoderSupport(): Promise<boolean> {
  try {
    return await canEncodeVideo('avc', {
      width: 1920,
      height: 1080,
      bitrate: DEFAULT_BITRATE,
    });
  } catch {
    return false;
  }
}

/**
 * Trim an AudioBuffer to a target duration in seconds.
 * If the buffer is shorter than the target, returns it unchanged.
 */
function trimAudioBuffer(buffer: AudioBuffer, targetDuration: number): AudioBuffer {
  const targetSamples = Math.floor(targetDuration * buffer.sampleRate);

  if (targetSamples >= buffer.length) {
    return buffer;
  }

  const trimmed = new AudioBuffer({
    length: Math.max(1, targetSamples),
    numberOfChannels: buffer.numberOfChannels,
    sampleRate: buffer.sampleRate,
  });

  for (let ch = 0; ch < buffer.numberOfChannels; ch++) {
    trimmed.getChannelData(ch).set(
      buffer.getChannelData(ch).subarray(0, targetSamples)
    );
  }

  return trimmed;
}

/**
 * Standalone async function to stitch multiple video blobs together sequentially
 */
export async function stitchVideosAsync(
  videoBlobs: Blob[],
  audioData?: AudioData | null,
  onProgress?: (progress: StitchProgress) => void
): Promise<Blob> {
  try {
    // Initialize progress
    const initialProgress: StitchProgress = {
      status: 'processing',
      message: 'Initializing stitching...',
      progress: 0,
      totalVideos: videoBlobs.length,
    };
    onProgress?.(initialProgress);

    if (videoBlobs.length === 0) {
      throw new Error('No videos to stitch');
    }

    // Helper to update progress
    const updateProgress = (
      status: StitchProgress['status'],
      message: string,
      progressValue: number,
      currentVideo?: number
    ) => {
      const p: StitchProgress = {
        status,
        message,
        progress: progressValue,
        currentVideo,
        totalVideos: videoBlobs.length,
      };
      onProgress?.(p);
    };

    updateProgress('processing', 'Analyzing video metadata...', 5);
    const {
      width: probedWidth,
      height: probedHeight,
      rotation: aggregateRotation,
      maxSourceBitrate,
    } = await determineEncodeParameters(videoBlobs);

    const safeWidth = probedWidth > 0 ? probedWidth : FALLBACK_WIDTH;
    const safeHeight = probedHeight > 0 ? probedHeight : FALLBACK_HEIGHT;

    // Cap bitrate to 8Mbps — inflated source bitrates cause encoder rejection
    const resolvedBitrate = DEFAULT_BITRATE;

    // Use only standard resolutions — hardware encoders reject non-standard
    // sizes like 1620x1080 even when canEncodeVideo() claims support.
    // Pick the smallest standard resolution that fits the source.
    const STANDARD_TIERS = [
      { w: 1920, h: 1080, codec: AVC_LEVEL_4_0, label: '1080p' },
      { w: 1280, h:  720, codec: AVC_LEVEL_4_0, label: '720p' },
    ] as const;

    let encodeWidth: number = STANDARD_TIERS[0].w;
    let encodeHeight: number = STANDARD_TIERS[0].h;
    let finalCodecProfile: string = STANDARD_TIERS[0].codec;
    let tierLabel: string = STANDARD_TIERS[0].label;

    // If source fits within 720p, use 720p; otherwise use 1080p
    if (safeWidth <= 1280 && safeHeight <= 720) {
      encodeWidth = 1280;
      encodeHeight = 720;
      finalCodecProfile = STANDARD_TIERS[1].codec;
      tierLabel = '720p';
    }

    // Verify the encoder actually supports it, fall back to 720p if not
    let encoderReady = await canEncodeVideo('avc', {
      width: encodeWidth,
      height: encodeHeight,
      bitrate: resolvedBitrate,
      fullCodecString: finalCodecProfile,
    });

    if (!encoderReady && encodeWidth > 1280) {
      encodeWidth = 1280;
      encodeHeight = 720;
      finalCodecProfile = AVC_LEVEL_4_0;
      tierLabel = '720p (fallback)';
      encoderReady = await canEncodeVideo('avc', {
        width: encodeWidth,
        height: encodeHeight,
        bitrate: resolvedBitrate,
        fullCodecString: finalCodecProfile,
      });
    }

    if (!encoderReady) {
      throw new Error(
        'Device encoder cannot output video. Try desktop Chrome or Firefox.'
      );
    }

    console.log('Stitch encoder configuration', {
      width: encodeWidth,
      height: encodeHeight,
      codecProfile: finalCodecProfile,
      bitrate: resolvedBitrate,
      tier: tierLabel,
      sourceResolution: `${safeWidth}x${safeHeight}`,
      rotation: aggregateRotation,
    });

    // Create output
    updateProgress('processing', `Creating output (${tierLabel}: ${encodeWidth}x${encodeHeight})...`, 10);

    let videoSource: VideoSampleSource | null = new VideoSampleSource(
      createAvcEncodingConfig(resolvedBitrate, encodeWidth, encodeHeight, finalCodecProfile)
    );

    const bufferTarget = new BufferTarget();
    let output: Output | null = new Output({
      format: new Mp4OutputFormat({ fastStart: 'in-memory' }),
      target: bufferTarget,
    });

    output.addVideoTrack(videoSource, { rotation: aggregateRotation, frameRate: MAX_OUTPUT_FPS });

    // Add audio track if provided
    let audioSource: AudioBufferSource | null = null;
    let pendingAudioBuffer: AudioBuffer | null = null;
    let outputStarted = false;

    if (audioData) {
      updateProgress('processing', 'Detecting supported audio codec...', 8);

      // Detect the best supported audio codec for MP4
      // Try common codecs in order of preference: aac, mp3 (no opus - Twitter doesn't support it)
      const audioCodec = await getFirstEncodableAudioCodec(['aac', 'mp3'], {
        numberOfChannels: audioData.buffer.numberOfChannels,
        sampleRate: audioData.buffer.sampleRate,
        bitrate: 128000,
      });

      if (!audioCodec) {
        console.warn('No supported audio codec found, continuing without audio');
      } else {
        updateProgress('processing', `Adding audio track (${audioCodec})...`, 10);

        // Create audio source from the decoded audio buffer
        audioSource = new AudioBufferSource({
          codec: audioCodec,
          bitrate: 128000,
        });

        output.addAudioTrack(audioSource);
        pendingAudioBuffer = audioData.buffer;
      }
    }

    await output.start();
    outputStarted = true;

    try {
    // Track the highest timestamp we've written to ensure monotonicity
    // Start at -frameInterval so first frame can be at timestamp 0
    const frameInterval = 1 / MAX_OUTPUT_FPS;
    let highestWrittenTimestamp = -frameInterval;

    // Process each video blob
    for (let videoIndex = 0; videoIndex < videoBlobs.length; videoIndex++) {
      const videoBlob = videoBlobs[videoIndex];
      const videoNumber = videoIndex + 1;

      updateProgress(
        'processing',
        `Processing video ${videoNumber}/${videoBlobs.length}...`,
        5 + (videoIndex / videoBlobs.length) * 90,
        videoNumber
      );

      // Create input for this video
      const blobSource = new BlobSource(videoBlob);
      const input = new Input({
        source: blobSource,
        formats: ALL_FORMATS,
      });

      try {
        const videoTracks = await input.getVideoTracks();
        if (videoTracks.length === 0) {
          console.warn(`No video tracks in video ${videoNumber}`);
          continue;
        }

        const videoTrack = videoTracks[0];
        const sink = new VideoSampleSink(videoTrack);

        // Get duration of this video
        const videoDuration = await input.computeDuration();

        // Track the base offset for this video segment
        const segmentBaseTime = highestWrittenTimestamp + frameInterval;
        // Track minimum timestamp in this segment to normalize
        let segmentMinTimestamp: number | null = null;

        // Read and write samples from this video
        let samplesFromThisVideo = 0;
        for await (const sample of sink.samples(0, videoDuration)) {
          const originalTimestamp = sample.timestamp ?? 0;

          // On first sample, record the minimum timestamp to normalize from
          if (segmentMinTimestamp === null) {
            segmentMinTimestamp = originalTimestamp;
          }

          // Normalize timestamp relative to segment start, then offset by base time
          const normalizedTimestamp = originalTimestamp - segmentMinTimestamp;
          const adjustedTimestamp = segmentBaseTime + normalizedTimestamp;

          // Snap to 60fps grid for consistent framerate
          const snappedTimestamp = Math.round(adjustedTimestamp / frameInterval) * frameInterval;

          // Skip duplicate frames that land on the same timestamp slot
          // This ensures strict 60fps without exceeding the target rate
          if (snappedTimestamp < highestWrittenTimestamp) {
            sample.close();
            continue;
          }

          sample.setTimestamp(snappedTimestamp);
          sample.setDuration(frameInterval);
          await videoSource!.add(sample);

          // Update highest written timestamp
          highestWrittenTimestamp = snappedTimestamp;

          sample.close();
          samplesFromThisVideo++;

          // Update progress
          if (samplesFromThisVideo % 10 === 0) {
            const videoProgress = samplesFromThisVideo / 300; // Rough estimate
            const overallProgress =
              5 + ((videoIndex + videoProgress) / videoBlobs.length) * 90;
            updateProgress(
              'processing',
              `Processing video ${videoNumber}/${videoBlobs.length}: ${samplesFromThisVideo} frames...`,
              overallProgress,
              videoNumber
            );
          }
        }
      } catch (videoError) {
        const errorMsg =
          videoError instanceof Error
            ? videoError.message
            : `Failed to process video ${videoNumber}`;
        console.error(`Error processing video ${videoNumber}:`, videoError);
        throw new Error(errorMsg);
      } finally {
        input.dispose();
      }
    }

    // Phase 2: Apply audio, trimmed to match the stitched video duration
    if (audioSource && pendingAudioBuffer) {
      updateProgress('processing', 'Encoding audio track...', 92);

      const videoDuration = highestWrittenTimestamp + frameInterval;
      const trimmedBuffer = trimAudioBuffer(pendingAudioBuffer, videoDuration);
      await audioSource.add(trimmedBuffer);
      await audioSource.close();
      audioSource = null; // Already closed
    }

    // Flush encoder before finalizing container
    await videoSource!.close();
    videoSource = null; // Already closed, prevent double-close in finally
    updateProgress('processing', 'Finalizing stitched video...', 97);

    // Finalize output
    await output!.finalize();
    outputStarted = false; // Successfully finalized
    output = null;
    const buffer = bufferTarget.buffer;

    if (!buffer) {
      throw new Error('Failed to generate output buffer');
    }

    const outputBlob = new Blob([buffer], { type: 'video/mp4' });

    updateProgress(
      'complete',
      `Successfully stitched ${videoBlobs.length} videos into ${(outputBlob.size / 1024 / 1024).toFixed(2)}MB file`,
      100
    );

    return outputBlob;
    } finally {
      // Clean up encoding resources on error
      pendingAudioBuffer = null;
      if (audioSource) {
        try {
          await audioSource.close();
        } catch (e) {
          console.warn('Failed to close audioSource:', e);
        }
      }
      if (videoSource) {
        try {
          await videoSource.close();
        } catch (e) {
          console.warn('Failed to close videoSource:', e);
        }
      }
      if (output && outputStarted) {
        try {
          await output.cancel();
        } catch (e) {
          console.warn('Failed to cancel output:', e);
        }
      }
    }
  } catch (error) {
    const normalizedError =
      error instanceof Error ? error : new Error(String(error));
    console.error('Video stitching error:', normalizedError);

    const errorProgress: StitchProgress = {
      status: 'error',
      message: `Error: ${normalizedError.message}`,
      progress: 0,
      error: normalizedError.message,
    };

    onProgress?.(errorProgress);

    throw normalizedError;
  }
}

interface UseStitchVideosReturn {
  stitchVideos: (
    videoBlobs: Blob[],
    audioData?: AudioData | null,
    onProgress?: (progress: StitchProgress) => void
  ) => Promise<Blob | null>;
  progress: StitchProgress;
  reset: () => void;
}

/**
 * Hook for stitching multiple video blobs together sequentially
 * Reads frames from each video and writes them to output in order
 */
export const useStitchVideos = (): UseStitchVideosReturn => {
  const [progress, setProgress] = useState<StitchProgress>({
    status: 'idle',
    message: 'Ready',
    progress: 0,
  });

  const stitchVideos = useCallback(
    async (
      videoBlobs: Blob[],
      audioData?: AudioData | null,
      onProgress?: (progress: StitchProgress) => void
    ): Promise<Blob | null> => {
      try {
        setProgress({
          status: 'processing',
          message: 'Initializing...',
          progress: 0,
        });

        const result = await stitchVideosAsync(videoBlobs, audioData, (p) => {
          setProgress(p);
          onProgress?.(p);
        });

        return result;
      } catch (error) {
        const errorMessage = error instanceof Error ? error.message : 'Video stitching failed';
        setProgress({
          status: 'error',
          message: errorMessage,
          progress: 0,
          error: errorMessage,
        });
        return null;
      }
    },
    []
  );

  const reset = useCallback(() => {
    setProgress({
      status: 'idle',
      message: 'Ready',
      progress: 0,
    });
  }, []);

  return {
    stitchVideos,
    progress,
    reset,
  };
};
